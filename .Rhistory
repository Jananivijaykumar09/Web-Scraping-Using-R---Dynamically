scrape_movie_recursive(link, paste0("D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/", basename(link), ".html"), visited_pages)
}
}
}
# URL of the movie page on IMDb
movie_url <- "https://www.imdb.com/title/tt0468569/"  # Example: The Dark Knight
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/movie_page.html"
# Call the function to scrape movie information and related pages
scrape_movie_recursive(movie_url, file_path)
library(rvest)
library(httr)
# Function to recursively scrape movie information and related pages
scrape_movie_recursive <- function(movie_url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (movie_url %in% visited_pages) {
cat("Already visited:", movie_url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, movie_url)
# Read the HTML content from the current page
page <- read_html(movie_url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", movie_url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to IMDb homepage
links <- links[!is.na(links)]
# Process related pages recursively
for (link in links) {
if (!is.na(link) && !str_detect(link, "^http")) {
link <- url_absolute(movie_url, link)
# Validate the URL
response <- GET(link)
if (length(response) == 0) {
cat("Error: Unable to access URL:", link, "\n")
next  # Skip to the next iteration if there's an issue with the HTTP request
}
# Check if the HTTP request is successful
if (http_status(response)$status_code != 200) {
cat("Invalid URL:", link, "\n")
next  # Skip to the next iteration if the URL is invalid
}
scrape_movie_recursive(link, paste0("D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/", basename(link), ".html"), visited_pages)
}
}
}
# URL of the movie page on IMDb
movie_url <- "https://www.imdb.com/title/tt0468569/"  # Example: The Dark Knight
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/movie_page.html"
# Call the function to scrape movie information and related pages
scrape_movie_recursive(movie_url, file_path)
library(rvest)
# Function to recursively scrape movie information and related pages
scrape_movie_recursive <- function(movie_url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (movie_url %in% visited_pages) {
cat("Already visited:", movie_url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, movie_url)
# Read the HTML content from the current page
page <- read_html(movie_url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", movie_url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to IMDb homepage
links <- links[!is.na(links)]
# Process related pages recursively
for (link in links) {
if (!is.na(link) && !str_detect(link, "^http")) {
link <- url_absolute(movie_url, link)
# Validate the URL
tryCatch({
page <- read_html(link)
if (is.null(page)) {
cat("Error: Unable to load the page:", link, "\n")
next  # Skip to the next iteration if unable to load the page
}
# Save the HTML content to a file
write_html(page, file = paste0("D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/", basename(link), ".html"))
# Recursively scrape the related page
scrape_movie_recursive(link, file_path, visited_pages)
}, error = function(e) {
cat("Error: Unable to access URL:", link, "\n")
})
}
}
}
# URL of the movie page on IMDb
movie_url <- "https://www.imdb.com/title/tt0468569/"  # Example: The Dark Knight
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/movie_page.html"
# Call the function to scrape movie information and related pages
scrape_movie_recursive(movie_url, file_path)
library(rvest)
# Function to recursively scrape movie information and related pages
scrape_movie_recursive <- function(movie_url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (movie_url %in% visited_pages) {
cat("Already visited:", movie_url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, movie_url)
# Read the HTML content from the current page
page <- read_html(movie_url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", movie_url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to IMDb homepage
links <- links[!is.na(links)]
# Process related pages recursively
for (link in links) {
if (!is.na(link) && !str_detect(link, "^http")) {
link <- url_absolute(movie_url, link)
# Validate the URL
tryCatch({
page <- read_html(link)
if (is.null(page)) {
cat("Error: Unable to load the page:", link, "\n")
next  # Skip to the next iteration if unable to load the page
}
# Save the HTML content to a file
write_html(page, file = paste0("D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/", basename(link), ".html"))
# Recursively scrape the related page
scrape_movie_recursive(link, file_path, visited_pages)
}, error = function(e) {
cat("Error: Unable to access URL:", link, "\n")
})
}
}
}
# URL of the movie page on IMDb
movie_url <- "https://www.imdb.com/title/tt0468569/"  # Example: The Dark Knight
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/movie_page.html"
# Call the function to scrape movie information and related pages
scrape_movie_recursive(movie_url, file_path)
library(rvest)
# Function to recursively scrape linked pages and save them as HTML files
scrape_linked_pages <- function(url, file_path) {
# Read the HTML content from the current page
page <- read_html(url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to the same page
links <- links[!is.na(links) & !str_detect(links, "^http") & !str_detect(links, "^#")]
# Process linked pages recursively
for (link in links) {
linked_url <- url_absolute(url, link)
linked_file_path <- paste0(file_path, "_", gsub("[^[:alnum:]]", "_", basename(linked_url)), ".html")
# Recursively scrape the linked page
scrape_linked_pages(linked_url, linked_file_path)
}
}
# URL of the webpage to scrape
url <- "https://example.com"  # Replace with the URL of the webpage you want to scrape
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/main_page.html"  # Adjust the path as needed
# Call the function to scrape linked pages recursively
scrape_linked_pages(url, file_path)
library(rvest)
# Function to recursively scrape linked pages and save them as HTML files
scrape_imdb_linked_pages <- function(url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (url %in% visited_pages) {
cat("Already visited:", url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, url)
# Read the HTML content from the current page
page <- read_html(url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to the same page
links <- links[!is.na(links) & !str_detect(links, "^http") & !str_detect(links, "^#")]
# Process linked pages recursively
for (link in links) {
linked_url <- url_absolute(url, link)
linked_file_path <- paste0(file_path, "_", gsub("[^[:alnum:]]", "_", basename(linked_url)), ".html")
# Recursively scrape the linked page
scrape_imdb_linked_pages(linked_url, linked_file_path, visited_pages)
}
}
# URL of the IMDb page to scrape
url <- "https://www.imdb.com/title/tt0468569/"  # Example: The Dark Knight
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/imdb_page.html"  # Adjust the path as needed
# Call the function to scrape linked pages recursively
scrape_imdb_linked_pages(url, file_path)
library(rvest)
# Function to recursively scrape linked pages and save them as HTML files
scrape_imdb_linked_pages <- function(url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (url %in% visited_pages) {
cat("Already visited:", url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, url)
# Read the HTML content from the current page
page <- read_html(url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to the same page
links <- links[!is.na(links) & !str_detect(links, "^http") & !str_detect(links, "^#")]
# Process linked pages recursively
for (link in links) {
linked_url <- url_absolute(url, link)
linked_file_path <- paste0(file_path, "_", gsub("[^[:alnum:]]", "_", basename(linked_url)), ".html")
# Recursively scrape the linked page
scrape_imdb_linked_pages(linked_url, linked_file_path, visited_pages)
}
}
# URL of the IMDb page to scrape
url <- "https://www.imdb.com/title/tt0468569/"  # The Dark Knight IMDb page
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/imdb_dark_knight_page"  # Adjust the path as needed
# Call the function to scrape linked pages recursively
scrape_imdb_linked_pages(url, file_path)
library(rvest)
# Function to recursively scrape linked pages and save them as HTML files
scrape_imdb_linked_pages <- function(url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (url %in% visited_pages) {
cat("Already visited:", url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, url)
# Read the HTML content from the current page
page <- read_html(url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to the same page
links <- links[!is.na(links) & !str_detect(links, "^http") & !str_detect(links, "^#")]
# Process linked pages recursively
for (link in links) {
linked_url <- url_absolute(url, link)
linked_file_path <- paste0(file_path, "_", gsub("[^[:alnum:]]", "_", basename(linked_url)), ".html")
# Recursively scrape the linked page
scrape_imdb_linked_pages(linked_url, linked_file_path, visited_pages)
# Update the HTML content of the current page with clickable links to the scraped linked webpages
page <- read_html(file_path)
linked_page_name <- gsub("[^[:alnum:]]", "_", basename(linked_file_path))
linked_page_relative_path <- paste0("./", basename(linked_file_path))
linked_page_link <- paste("<a href='", linked_page_relative_path, "'>", linked_page_name, "</a>", sep = "")
page <- page %>%
html_node("body") %>%
html_add(html(linked_page_link))
# Save the updated HTML content
write_html(page, file = file_path)
}
}
# URL of the IMDb page to scrape
url <- "https://www.imdb.com/title/tt0468569/"  # The Dark Knight IMDb page
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/imdb_dark_knight_page.html"  # Adjust the path as needed
# Call the function to scrape linked pages recursively
scrape_imdb_linked_pages(url, file_path)
library(rvest)
# Function to recursively scrape linked pages and save them as HTML files
scrape_imdb_linked_pages <- function(url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (url %in% visited_pages) {
cat("Already visited:", url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, url)
# Read the HTML content from the current page
page <- read_html(url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to the same page
links <- links[!is.na(links) & !str_detect(links, "^http") & !str_detect(links, "^#")]
# Process linked pages recursively
for (link in links) {
linked_url <- url_absolute(url, link)
linked_file_path <- paste0(file_path, "_", gsub("[^[:alnum:]]", "_", basename(linked_url)), ".html")
# Recursively scrape the linked page
scrape_imdb_linked_pages(linked_url, linked_file_path, visited_pages)
# Append a link to the linked page in the main page
linked_page_name <- gsub("[^[:alnum:]]", "_", basename(linked_file_path))
linked_page_relative_path <- paste0(basename(linked_file_path))
linked_page_link <- paste("<a href='", linked_page_relative_path, "'>", linked_page_name, "</a><br>", sep = "")
write(linked_page_link, file = file_path, append = TRUE)
}
}
# URL of the IMDb page to scrape
url <- "https://www.imdb.com/title/tt0468569/"  # The Dark Knight IMDb page
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/imdb_dark_knight_page.html"  # Adjust the path as needed
# Call the function to scrape linked pages recursively
scrape_imdb_linked_pages(url, file_path)
library(rvest)
# Function to recursively scrape linked pages and save them as HTML files
scrape_imdb_linked_pages <- function(url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (url %in% visited_pages) {
cat("Already visited:", url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, url)
# Read the HTML content from the current page
page <- read_html(url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Filter out external links and links to the same page
links <- links[!is.na(links) & !str_detect(links, "^http") & !str_detect(links, "^#")]
# Process linked pages recursively
for (link in links) {
linked_url <- url_absolute(url, link)
linked_file_path <- paste0(file_path, "_", gsub("[^[:alnum:]]", "_", basename(linked_url)), ".html")
# Recursively scrape the linked page
scrape_imdb_linked_pages(linked_url, linked_file_path, visited_pages)
}
}
# URL of the IMDb page to scrape
url <- "https://www.imdb.com/title/tt0468569/"  # The Dark Knight IMDb page
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/imdb_dark_knight_page.html"  # Adjust the path as needed
# Call the function to scrape linked pages recursively
scrape_imdb_linked_pages(url, file_path)
library(rvest)
# Function to recursively scrape linked pages and save them as HTML files
scrape_imdb_linked_pages <- function(url, file_path, visited_pages = character(0)) {
# Check if the URL has already been visited to avoid infinite loops
if (url %in% visited_pages) {
cat("Already visited:", url, "\n")
return()
}
# Add the current URL to the list of visited pages
visited_pages <- c(visited_pages, url)
# Read the HTML content from the current page
page <- read_html(url)
# Check if the page is successfully loaded
if (is.null(page)) {
cat("Error: Unable to load the page:", url, "\n")
return()
}
# Save the HTML content to the specified file path
write_html(page, file = file_path)
# Print a message indicating where the file is saved
cat("The HTML content of the page has been saved to:", file_path, "\n")
# Extract links from the page
links <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
unique() %>%
str_trim()
# Process linked pages recursively
for (link in links) {
# Check if the link is valid and not already visited
if (!is.na(link) && !str_detect(link, "^#")) {
linked_url <- url_absolute(url, link)
linked_file_path <- paste0(file_path, "_", gsub("[^[:alnum:]]", "_", basename(linked_url)), ".html")
# Recursively scrape the linked page
scrape_imdb_linked_pages(linked_url, linked_file_path, visited_pages)
}
}
}
# URL of the IMDb page to scrape
url <- "https://www.imdb.com/title/tt0468569/"  # The Dark Knight IMDb page
# Specify the file path to save the HTML content
file_path <- "D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/imdb_dark_knight_page.html"  # Adjust the path as needed
# Call the function to scrape linked pages recursively
scrape_imdb_linked_pages(url, file_path)
# Install and load necessary libraries
install.packages("RSelenium")
library(RSelenium)
library(rvest)
# Start a Selenium server
rsDriver(browser = "chrome")
# Install and load necessary libraries
install.packages("RSelenium")
library(RSelenium)
library(rvest)
# Start a Selenium server
rsDriver(browser = "chrome")
# Install and load necessary libraries
install.packages("RSelenium")
library(RSelenium)
library(rvest)
# Start a Selenium server
rsDriver(browser = "chrome")
# Install and load necessary libraries
install.packages("RSelenium")
library(RSelenium)
library(rvest)
# Start a Selenium server using ChromeDriver
rD <- rsDriver(browser = "chrome", chromever = "latest", port = 4445L)
library(shiny); runApp('D:/SEMESTER 2/OSS/UNIT 2/23MX109_U2_EX1/R_package.R')
